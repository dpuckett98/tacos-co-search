import matplotlib.pyplot as plt

def show_attention_score_by_bandwidth(with_preload=True):
	bws = [10, 20, 30, 40, 50, 60, 70, 80]
	if with_preload:
		bws = [5, 10, 20, 30, 40, 50, 60, 70, 80]
		cycles_single = [156180, 66096, 29184, 21276, 17304, 14928, 13356, 12216, 11364]
		cycles_overlapped = [192932, 95496, 47261, 31175, 23139, 18334, 15103, 12820, 11192]
		cycles_decoder = [124293, 60644, 30578, 20628, 16383, 14039, 12481, 11717, 11634]
	else:
		cycles_single = [78096, 39048, 26676, 20868, 17412, 15096, 13440, 12252]
		cycles_overlapped = [96496, 48261, 32175, 24139, 19334, 16103, 13820, 12082]
		cycles_decoder = [62644, 31506, 21128, 16459, 14101, 12533, 11761, 11672]
		#cycles_single = [20112, 8052, 8052, 8052, 8052, 8052, 8052, 8052]
		#cycles_overlapped = [95512, 47268, 31206, 23168, 18347, 15122, 12833, 11225]
		#cycles_decoder = [27850, 14117, 11471, 10356, 9958, 9765, 9438, 9402]
	
	plt.plot(bws, cycles_single, '.-', label="Without enc/dec")
	#plt.plot(bws, cycles_overlapped, '.-', label="Overlapped")
	plt.plot(bws, cycles_decoder, '.-', label="With enc/dec")
	plt.legend()
	plt.xlabel("Bandwidth")
	plt.ylabel("Cycles")
	#plt.xscale("log")
	#plt.yscale("log")
	plt.title("Cycles to Compute the Attention Score of Deit-Base Under 90% Sparsity Using Various Methods")
	plt.show()

def show_best_number_of_PEs(bw=77):
	if bw == 77:
		num_PEs = [i*8 for i in [30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200]]
		# these have no pipelining
		without_enc_dec = [19812, 17412, 15636, 14688, 13944, 13320, 12864, 12528, 12252, 12036, 11844, 11676, 11520, 11388, 11268, 11160, 11064, 10980, 10896, 10812, 10740, 10680, 10620, 10572, 10512, 10464, 10416, 10380, 10332, 10296, 10260, 10236, 10200, 10164, 10164]
		with_enc_dec = [23542, 20032, 17488, 15778, 14865, 12740, 12056, 11508, 10891, 10575, 9590, 9281, 9271, 9261, 9241, 9233, 9212, 9205, 9189, 9182, 9182, 9053, 9053, 9048, 9047, 9040, 8310, 8297, 8292, 8292, 8286, 8286, 8286, 8285, 8281]
		# these have preloading, but no pipelined offloading
		#num_PEs = [i * 8 for i in [30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200]]
		#without_enc_dec = [18996, 16572, 14820, 13860, 13116, 12492, 11964, 11508, 11160, 10836, 10536, 10272, 10068, 9864, 9672, 9528, 9372, 9228, 9120, 9012, 8892, 8808, 8688, 8616, 8544, 8484, 8412, 8352, 8280, 8220, 8160, 8100, 8076, 8028, 7968]
		#with_enc_dec = [23528, 20004, 17454, 15750, 14851, 12688, 12016, 11468, 10863, 10561, 9528, 9203, 9193, 9183, 9175, 9167, 9160, 9153, 9147, 9140, 9140, 9011, 9011, 9004, 9003, 8996, 8126, 8083, 8078, 8078, 8072, 8072, 8072, 8071, 8067]
	elif bw == 10:
		num_PEs = [i * 8 for i in [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]]
		# no pipelining
		without_enc_dec = [122784, 92472, 84864, 81060, 78816, 78096, 78096, 78096, 78096, 78096, 78096, 78096]
		with_enc_dec = [134249, 71802, 63807, 62997, 62858, 62783, 62732, 62690, 62659, 62634, 62612, 62596]
		# with preloading, but no pipelined offloading
		#num_PEs = [i * 8 for i in [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 75, 100, 125, 150, 175, 200]]
		#without_enc_dec = [116688, 81804, 72864, 69060, 66816, 66096, 66096, 66096, 66096, 66096, 66096, 66096, 66096, 66096, 66096, 66096, 66096, 66096]
		#with_enc_dec = [134147, 71694, 63669, 61413, 61124, 60927, 60754, 60690, 60659, 60634, 60612, 60596, 13358, 13358, 13358, 13358, 13358, 13358]
	
	plt.plot(num_PEs, without_enc_dec, '.-', label="Without enc/dec")
	plt.plot(num_PEs, with_enc_dec, '.-', label="With enc/dec")
	plt.legend()
	plt.xlabel("Number of PEs")
	plt.ylabel("Cycles")
	#plt.xscale("log")
	#plt.yscale("log")
	plt.title("Cycles to Compute the Attention Score of Deit-Base Under 90% Sparsity With and Without the Enc/Dec at " + str(bw) + " GB/s")
	plt.show()

def plot_attention_score_by_bandwidth(model="deit-tiny", pipelining=False, num_PEs=512):
	if model == 'deit-tiny' and pipelining == False and num_PEs == 512:
		bws = [10, 20, 30, 40, 50, 60, 70, 80]
		dense = [24126, 18036, 17208, 16794, 16539, 16377, 16254, 16167]
		sparse = [18576, 9132, 6126, 4632, 3951, 3444, 3042, 2748]
		tacos = [14124, 7196, 4854, 3779, 3124, 2685, 2410, 2564]
	elif model == 'deit-tiny' and pipelining == True and num_PEs == 512:
		bws = [10, 20, 30, 40, 50, 60, 70, 80]
		dense = [15522, 15522, 15522, 15522, 15522, 15522, 15522, 15522]
		sparse = [4974, 1710, 1812, 1788, 1722, 1731, 1815, 1842]
		tacos = [3941, 2834, 2568, 2349, 2227, 2198, 2247, 2113]
	elif model == 'deit-tiny' and pipelining == True and num_PEs == 512 + 16*8:
		bws = [10, 20, 30, 40, 50, 60, 70, 80]
		dense = [12660, 12660, 12660, 12660, 12660, 12660, 12660, 12660]
		sparse = [4890, 1386, 1563, 1545, 1584, 1377, 1485, 1377]
		tacos = [3611, 2609, 2249, 1964, 1935, 1883, 1854, 1813]
	elif model == 'deit-tiny' and pipelining == True and num_PEs == 512 + 32*8:
		bws = [10, 20, 30, 40, 50, 60, 70, 80]
		dense = [10698, 10698, 10698, 10698, 10698, 10698, 10698, 10698]
		sparse = [4761, 1341, 1269, 1458, 1191, 1317, 1230, 1395]
		tacos = [3370, 2329, 1973, 1818, 1755, 1630, 1639, 1624]
	else:
		print("Invalid inputs:", model, pipelining, num_PEs)
	
	#plt.plot(bws, dense, '.-', label="Dense")
	plt.plot(bws, sparse, '.-', label="Sparse")
	plt.plot(bws, tacos, '.-', label="Sparse + Enc/Dec")
	plt.legend()
	plt.xlabel("Bandwidth (GB/s)")
	plt.ylabel("Cycles")
	#plt.xscale("log")
	#plt.yscale("log")
	plt.title("Cycles to Compute the Attention Score of " + model)
	plt.show()

def plot_best_number_pes(model="deit-tiny", bw=10):
	pes = [8*i for i in [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200]]
	
	# deit-tiny with 10 GB/s
	if model == 'deit-tiny' and bw == 10:
		names = ["embeddings, linear #2", "Q, K, V, MLP", "Score (with enc/dec)", "Score*V", "Linear #1"]
		layer_0 = [372140, 187368, 126047, 94859, 76995, 65696, 59488, 54913, 51237, 49542, 49531, 49531, 49531, 49531, 49531, 49531, 49531, 49531, 49531, 49531]
		layer_1 = [94310, 48213, 32905, 25204, 20882, 18133, 16997, 16269, 15754, 15208, 15208, 15208, 15208, 15208, 15208, 15208, 15208, 15208, 15208, 15208]
		#layer_2 = [94310, 48213, 32905, 25204, 20882, 18133, 16997, 16269, 15754, 15208, 15208, 15208, 15208, 15208, 15208, 15208, 15208, 15208, 15208, 15208]
		layer_3 = [12535, 12303, 12279, 12223, 12671, 12351, 12375, 12455, 12623, 12503, 12463, 12559, 12655, 12543, 12423, 12703, 12703, 12543, 12207, 12215]
		#layer_4 = [94310, 48213, 32905, 25204, 20882, 18133, 16997, 16269, 15754, 15208, 15208, 15208, 15208, 15208, 15208, 15208, 15208, 15208, 15208, 15208]
		layer_5 = [6645, 6433, 6423, 6474, 6435, 6418, 6443, 6448, 6483, 6533, 6364, 6437, 6358, 6417, 6513, 6351, 6395, 6443, 6483, 6363]
		#layer_6 = [94310, 48213, 32905, 25204, 20882, 18133, 16997, 16269, 15754, 15208, 15208, 15208, 15208, 15208, 15208, 15208, 15208, 15208, 15208, 15208]
		layer_7 = [373269, 188404, 126846, 95862, 77996, 65889, 57449, 54735, 53476, 53148, 53148, 53148, 53148, 53148, 53148, 53148, 53148, 53148, 53148, 53148]
		#layer_8 = [372140, 187368, 126047, 94859, 76995, 65696, 59488, 54913, 51237, 49542, 49531, 49531, 49531, 49531, 49531, 49531, 49531, 49531, 49531, 49531]
		#layer_9 = [21239, 21239, 21239, 21239, 21239, 21239, 21239, 21239, 21239, 21239, 21239, 21239, 21239, 21239, 21239, 21239, 21239, 21239, 21239, 21239]
		decode_q = [2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472]
		decode_k = [2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472]
		score = [23626, 22216, 22417, 22249, 22603, 22323, 22406, 21814, 21954, 22418, 22122, 21820, 21916, 21906, 22052, 22289, 22126, 22315, 22185, 22295]
		layer_3 = [i + j + k for i, j, k in zip(decode_q, decode_k, score)]
	elif model == 'deit-tiny' and bw == 5:
		# attention layers are wrong lol
		names = ["embeddings, linear #2", "Q, K, V, MLP", "Score (no enc/dec)", "Score*V", "Linear #1", "Score (with enc/dec)"]
		layer_0 = [373017, 188298, 128095, 103486, 98797, 98751, 98713, 98713, 98713, 98713, 98713, 98713, 98713, 98713, 98713, 98713, 98713, 98713, 98713, 98713]
		layer_1 = [95186, 49143, 34953, 30621, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413]
		layer_2 = [95186, 49143, 34953, 30621, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413]
		layer_3 = [22818, 22972, 22341, 22650, 22275, 22624, 22340, 22398, 22541, 22137, 22365, 22237, 22618, 22129, 22012, 22390, 22427, 21824, 22139, 21964]
		layer_4 = [95186, 49143, 34953, 30621, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413]
		layer_5 = [13019, 12854, 12915, 12826, 12851, 12944, 12672, 12866, 12986, 12958, 12808, 12784, 12746, 12834, 12918, 12864, 12834, 12846, 12678, 12866]
		layer_6 = [95186, 49143, 34953, 30621, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413, 30413]
		layer_7 = [374346, 189534, 129094, 106970, 106292, 106292, 106292, 106292, 106292, 106292, 106292, 106292, 106292, 106292, 106292, 106292, 106292, 106292, 106292, 106292]
		layer_8 = [373017, 188298, 128095, 103486, 98797, 98751, 98713, 98713, 98713, 98713, 98713, 98713, 98713, 98713, 98713, 98713, 98713, 98713, 98713, 98713]
		layer_9 = [42458, 42458, 42458, 42458, 42458, 42458, 42458, 42458, 42458, 42458, 42458, 42458, 42458, 42458, 42458, 42458, 42458, 42458, 42458, 42458]
		decode_q = [2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472]
		decode_k = [2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472]
		score = [23358, 22493, 22390, 22314, 22629, 22295, 22339, 21874, 22183, 22405, 22573, 22588, 21795, 21938, 21754, 22237, 22115, 22116, 22069, 22146]
		score_enc_dec = [i + j + k for i, j, k in zip(decode_q, decode_k, score)]
	elif model == 'deit-tiny' and bw == 20:
		names = ["embeddings, linear #2", "Q, K, V, MLP", "Score (with enc/dec)", "Score*V", "Linear #1"]
		layer_0 = [371327, 186498, 125149, 93945, 75877, 63685, 55105, 48243, 43082, 39334, 36253, 33304, 31692, 30385, 28953, 28112, 26974, 25968, 25618, 24892]
		layer_1a = [92918, 46763, 31429, 23711, 19185, 16152, 13954, 12271, 11012, 10132, 9452, 8645, 8239, 7804, 7415, 6963, 6737, 6390, 6198, 5968]
		layer_1b = [667, 649, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631]
		layer_2a = [92918, 46763, 31429, 23711, 19185, 16152, 13954, 12271, 11012, 10132, 9452, 8645, 8239, 7804, 7415, 6963, 6737, 6390, 6198, 5968]
		layer_2b = [667, 649, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631, 631]
		layer_3a = [641, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635]
		layer_3b = [641, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635]
		layer_3c = [11139, 6752, 6460, 6101, 5960, 5874, 5909, 5844, 5886, 5750, 5875, 5770, 5836, 5582, 5714, 5687, 5712, 5599, 5747, 5625]
		layer_4 = [93497, 47342, 32008, 24290, 19764, 16731, 14533, 12850, 11591, 10711, 10031, 9224, 8818, 8435, 8481, 8329, 8205, 7823, 7967, 7887]
		layer_5 = [4432, 3251, 3258, 3271, 3238, 3287, 3269, 3229, 3251, 3262, 3277, 3241, 3267, 3287, 3240, 3277, 3219, 3249, 3262, 3277]
		layer_6 = [93497, 47342, 32008, 24290, 19764, 16731, 14533, 12850, 11591, 10711, 10031, 9224, 8818, 8435, 8481, 8329, 8205, 7823, 7967, 7887]
		layer_7 = [372057, 187134, 125549, 94548, 76478, 64087, 55456, 48744, 43529, 39733, 36695, 33330, 31568, 29622, 27829, 27551, 27391, 27184, 26703, 26979]
		layer_8 = [371327, 186498, 125149, 93945, 75877, 63685, 55105, 48243, 43082, 39334, 36253, 33304, 31692, 30385, 28953, 28112, 26974, 25968, 25618, 24892]
		layer_9 = [19974, 10623, 10623, 10623, 10623, 10623, 10623, 10623, 10623, 10623, 10623, 10623, 10623, 10623, 10623, 10623, 10623, 10623, 10623, 10623]
		
		layer_1 = [i+j for i, j in zip(layer_1a, layer_1b)]
		layer_2 = [i+j for i, j in zip(layer_2a, layer_2b)]
		layer_3 = [i+j+k for i, j, k in zip(layer_3a, layer_3b, layer_3c)]
	elif model == 'deit-tiny' and bw == 30:
		names = ["embeddings, linear #2", "Q, K, V, MLP", "Score (with enc/dec)", "Score*V", "Linear #1"]
		layer_0 = [371093, 186231, 124872, 93651, 75575, 63378, 54791, 47915, 42741, 38947, 35801, 32722, 30425, 28638, 26579, 25028, 23889, 22833, 22059, 21120]
		layer_1a = [92877, 46689, 31344, 23610, 19076, 16038, 13833, 12136, 10864, 9938, 9194, 8332, 7888, 7412, 7009, 6563, 6331, 5975, 5782, 5512]
		layer_1b = [586, 445, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427]
		layer_2a = [92877, 46689, 31344, 23610, 19076, 16038, 13833, 12136, 10864, 9938, 9194, 8332, 7888, 7412, 7009, 6563, 6331, 5975, 5782, 5512]
		layer_2b = [586, 445, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427, 427]
		layer_3a = [575, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429]
		layer_3b = [575, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429, 429]
		layer_3c = [11348, 6227, 4550, 4326, 4225, 4090, 4039, 4048, 3968, 3886, 3899, 3872, 3891, 3812, 3912, 3849, 3894, 3821, 3736, 3813]
		layer_4 = [93263, 47075, 31730, 23996, 19462, 16424, 14219, 12522, 11250, 10324, 9580, 8718, 8274, 7798, 7395, 6949, 6717, 6361, 6168, 6054]
		layer_5 = [4497, 2395, 2231, 2213, 2203, 2175, 2199, 2206, 2171, 2160, 2191, 2185, 2209, 2161, 2174, 2160, 2174, 2196, 2178, 2167]
		layer_6 = [93263, 47075, 31730, 23996, 19462, 16424, 14219, 12522, 11250, 10324, 9580, 8718, 8274, 7798, 7395, 6949, 6717, 6361, 6168, 6054]
		layer_7 = [371690, 186734, 125138, 94121, 76043, 63647, 55009, 48284, 43055, 39213, 36111, 32692, 30891, 28904, 27096, 25496, 24108, 22903, 22400, 20973]
		layer_8 = [371093, 186231, 124872, 93651, 75575, 63378, 54791, 47915, 42741, 38947, 35801, 32722, 30425, 28638, 26579, 25028, 23889, 22833, 22059, 21120]
		layer_9 = [19947, 10139, 7114, 7097, 7097, 7097, 7097, 7097, 7097, 7097, 7097, 7097, 7097, 7097, 7097, 7097, 7097, 7097, 7097, 7097]
		
		layer_1 = [i+j for i, j in zip(layer_1a, layer_1b)]
		layer_2 = [i+j for i, j in zip(layer_2a, layer_2b)]
		layer_3 = [i+j+k for i, j, k in zip(layer_3a, layer_3b, layer_3c)]
	else:
		print("Invalid inputs:", model, bw)
		return
	
	for name, layer in zip(names, [layer_0, layer_1, layer_3, layer_5, layer_7]):
		speedups = []
		for i in layer:
			speedups.append(i / layer[-1])
		plt.plot(pes, speedups, '.-', label=name)
	
	plt.legend()
	plt.title(model + " w/ 90% sparsity and " + str(bw) + " GB/s bandwidth")
	plt.yscale("log")
	plt.xlabel("Number of PEs")
	plt.ylabel("Speedup over PEs = " + str(200*8))
	plt.show()

plot_attention_score_by_bandwidth(pipelining=True)
#plot_best_number_pes("deit-tiny", 30)
#show_best_number_of_PEs(10)
#show_attention_score_by_bandwidth(False)